{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize your local configuration\n",
    "\n",
    "Create a .env file in this folder and configure your local path to following properties: \n",
    "\n",
    "```\n",
    "ROOT_PATH =                     # path to root folder where alignment result to process as well output benchmark directory is located\n",
    "ALIGNMENT_RESULTS_FOLDER_NAME = # folder name with alignment results \n",
    "BENCHMARK_FOLDER_NAME  =        # folder name with benchmark results\n",
    "\n",
    "DATASET_WIKI =                  # path to wiki-manual\n",
    "DATASET_NEWSELA =               # path to newsela-manual\n",
    "DATASET_SICK =                  # path to Sick\n",
    "DATASET_STS12 =                 # path to STS\n",
    "DATASET_STS13 =                 # path to STS\n",
    "DATASET_STS14 =                 # path to STS\n",
    "DATASET_STS15 =                 # path to STS\n",
    "DATASET_STS16 =                 # path to STS\n",
    "DATASET_STSBENCHMARK =          # path to STS Benchmark\n",
    "DATASET_RTE1_CD =               # path to RTE CD\n",
    "DATASET_RTE1_PP =               # path to RTE PP\n",
    "DATASET_PARA =                  # path to PARA\n",
    "DATASET_ONESTOPENGLISH =        # path to ONESTOPENGLISH   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import  load_dotenv\n",
    "load_dotenv()\n",
    "ROOT_PATH = os.getenv('ROOT_PATH')                                          \n",
    "ALIGNMENT_RESULTS_FOLDER_NAME = os.getenv('ALIGNMENT_RESULTS_FOLDER_NAME')   \n",
    "BENCHMARK_FOLDER_NAME = os.getenv('BENCHMARK_FOLDER_NAME')              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_WIKI  = os.getenv('DATASET_WIKI')\n",
    "DATASET_NEWSELA  = os.getenv('DATASET_NEWSELA') \n",
    "DATASET_SICK  = os.getenv('DATASET_SICK') \n",
    "DATASET_STS12  = os.getenv('DATASET_STS12') \n",
    "DATASET_STS13  = os.getenv('DATASET_STS13') \n",
    "DATASET_STS14  = os.getenv('DATASET_STS14') \n",
    "DATASET_STS15  = os.getenv('DATASET_STS15') \n",
    "DATASET_STS16  = os.getenv('DATASET_STS16') \n",
    "DATASET_STSBENCHMARK  = os.getenv('DATASET_STSBENCHMARK') \n",
    "DATASET_RTE1_CD  = os.getenv('DATASET_RTE1_CD') \n",
    "DATASET_RTE1_PP  = os.getenv('DATASET_RTE1_PP') \n",
    "DATASET_PARA  = os.getenv('DATASET_PARA') \n",
    "DATASET_ONESTOPENGLISH  = os.getenv('DATASET_ONESTOPENGLISH')   \n",
    "\n",
    "TEST_DATA_SETS_DETAILS = {\n",
    "    \"wiki\":[os.path.join(DATASET_WIKI, \"test.tsv\"), (0,3,4)], \n",
    "    \"newsela\":[os.path.join(DATASET_NEWSELA, \"test.tsv\"), (0,3,4)],\n",
    "    \"sick\":[os.path.join(DATASET_SICK, \"sick_test_alignment.txt\"),  (0,1,2)],\n",
    "    \"sts12\":[os.path.join(DATASET_STS12, \"mteb_sts12-sts.txt\"),  (0,1,2)],\n",
    "    \"sts13\":[os.path.join(DATASET_STS13, \"mteb_sts13-sts.txt\"),  (0,1,2)],\n",
    "    \"sts14\":[os.path.join(DATASET_STS14, \"mteb_sts14-sts.txt\"),  (0,1,2)],\n",
    "    \"sts15\":[os.path.join(DATASET_STS15, \"mteb_sts15-sts.txt\"),  (0,1,2)],\n",
    "    \"sts16\":[os.path.join(DATASET_STS16, \"mteb_sts16-sts.txt\"),  (0,1,2)], \n",
    "    \"stsbenchmark\":[os.path.join(DATASET_STSBENCHMARK, \"mteb_stsbenchmark-sts.txt\"),  (0,1,2)], \n",
    "    \"rte1-cd\":[os.path.join(DATASET_RTE1_CD, \"rte1_cd.txt\"),  (0,1,2)],  \n",
    "    \"rte1-pp\":[os.path.join(DATASET_RTE1_PP, \"rte1_pp.txt\"),  (0,1,2)], \n",
    "    \"para\" :[os.path.join(DATASET_PARA, \"para_test.txt\"),  (0,1,2)],  \n",
    "    \"OneStopEnglish\" :[os.path.join(DATASET_ONESTOPENGLISH, \"OneStopEnglishCorpus_Sentence-Aligned.txt\"), (0,1,2)]     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_264671/1465959199.py:70: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for txt_file in  tqdm.tqdm_notebook(files_to_process):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df811b76f6b649af898a1e2fdd1c7907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68247 to remove\n",
      "DS stats\n",
      "Total nbr of records: 18774\n",
      "\tONESTOPENGLISH -> 156\n",
      "\tNEWSELA -> 1412\n",
      "\tPARA -> 838\n",
      "\tRTE1-CD -> 109\n",
      "\tRTE1-PP -> 33\n",
      "\tSICK -> 3827\n",
      "\tSTS12 -> 2394\n",
      "\tSTS13 -> 1054\n",
      "\tSTS14 -> 3373\n",
      "\tSTS15 -> 1910\n",
      "\tSTS16 -> 890\n",
      "\tSTSBENCHMARK -> 1133\n",
      "\tWIKI -> 1646\n",
      "Nbr of errors: 0\n",
      "Benchmark done \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pathlib\n",
    "import tqdm\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path   \n",
    "path_to_files = os.path.join(ROOT_PATH, ALIGNMENT_RESULTS_FOLDER_NAME)\n",
    "random.seed(10)\n",
    "\n",
    "def read_ds(path_to_ds, index_label=0, index_src=3, index_trg=4, previous_version=False, return_full_line=False):\n",
    "    \n",
    "    source = []\n",
    "    target = []\n",
    "    aligned = []\n",
    "    full_lines = []\n",
    "    is_eof = False\n",
    "    if not previous_version:\n",
    "        with open(path_to_ds, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if len(line.strip()) > 0:\n",
    "                    items = line.split(\"\\t\")\n",
    "                    aligned.append(items[index_label].strip())\n",
    "                    source.append(items[index_src].strip())\n",
    "                    target.append(items[index_trg].strip())\n",
    "                    full_lines.append(line.strip())\n",
    "    else:\n",
    "        file = open(path_to_ds, 'r', encoding='utf-8')\n",
    "        while not is_eof:\n",
    "            line = file.readline()\n",
    "            if len(line.strip()) > 0:\n",
    "                items = line.split(\"\\t\")\n",
    "                aligned.append(items[index_label].strip())\n",
    "                source.append(items[index_src].strip())\n",
    "                target.append(items[index_trg].strip())\n",
    "                full_lines.append(line.strip())\n",
    "            else:\n",
    "                is_eof = True \n",
    "    if return_full_line:\n",
    "        return source, target, aligned, full_lines\n",
    "    else:\n",
    "        return source, target, aligned    \n",
    "\n",
    "def split_filename(file_name):\n",
    "    file_name = file_name.split(\".\")[0]\n",
    "    return file_name.split(\"_\")\n",
    "\n",
    "def read_unique(path_to_files, index_label, index_src, index_trg):\n",
    "    sentences = []\n",
    "    for path_to_file in path_to_files:\n",
    "        source, target, aligned = read_ds(path_to_file, index_label=index_label, index_src=index_src, index_trg=index_trg)\n",
    "        for index in range(len(source)):\n",
    "            sentences.append(f'{aligned[index].strip()}\\t{source[index].strip()}\\t{target[index].strip()}')\n",
    "    uniqe = set(sentences)\n",
    "    return uniqe  \n",
    "\n",
    "def fix_crf_file_name(path_to_files):\n",
    "    files_to_process = sorted(pathlib.Path(path_to_files).glob('*_chaojiang06_*.txt'))\n",
    "    for fileName in files_to_process:\n",
    "        os.rename(fileName, str(fileName).replace(\"_chaojiang06_\", \"_chaojiang06-\"))\n",
    "    files_to_process = sorted(pathlib.Path(path_to_files).glob('*_rte1_*.txt'))\n",
    "    for fileName in files_to_process:\n",
    "        os.rename(fileName, str(fileName).replace(\"_rte1_\", \"_rte1-\"))   \n",
    "    \n",
    "            \n",
    "def get_negatives(path_to_files):\n",
    "    output = {}\n",
    "    negatives = []\n",
    "    current = set()\n",
    "    files_to_process = sorted(pathlib.Path(path_to_files).glob('*.txt'))\n",
    "    for txt_file in  tqdm.tqdm_notebook(files_to_process):\n",
    "        file_name = Path(txt_file).name\n",
    "        file_name_attr = split_filename(file_name)\n",
    "        method, ds, error_type = file_name_attr[1], file_name_attr[-2], file_name_attr[-1]\n",
    "        lines = []\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                lines.append(line.strip())    \n",
    "        uniqlines = set(lines)\n",
    "        to_add = uniqlines.difference(current)\n",
    "        output[\"\\t\"+method.upper()+\"\\t\"+ds.upper()+\"\\t\"+error_type.upper()] = to_add\n",
    "        current.update(to_add)\n",
    "            \n",
    "    for item in output.keys():\n",
    "        lines = output[item]\n",
    "        for line in lines: \n",
    "            line = line.strip()\n",
    "            if len(line)>0:\n",
    "                negatives.append(line+item)\n",
    "    return negatives\n",
    "\n",
    "def get_positives(path_to_files, percentTPFN=0.05):\n",
    "    ds_set = {}\n",
    "    ds_fptn_set = {}\n",
    "    all_records = []\n",
    "    for ds in TEST_DATA_SETS_DETAILS.keys():\n",
    "        unique_ds =  read_unique([TEST_DATA_SETS_DETAILS[ds][0]], index_label=TEST_DATA_SETS_DETAILS[ds][1][0], index_src=TEST_DATA_SETS_DETAILS[ds][1][1], index_trg=TEST_DATA_SETS_DETAILS[ds][1][2])\n",
    "        ds_set[ds] = unique_ds\n",
    "        all_records.extend(list(unique_ds))\n",
    "        \n",
    "        file_paths =  glob.glob(os.path.join(path_to_files, f'*{ds}_F*.txt'))\n",
    "        unique_ds_fptn = read_unique(file_paths, index_label=0, index_src=1, index_trg=2)\n",
    "        ds_fptn_set[ds] = unique_ds_fptn\n",
    "\n",
    "            \n",
    "\n",
    "    positives = []\n",
    "    for key in ds_set.keys():\n",
    "        remainsentences = list(ds_fptn_set[key] ^ ds_set[key])\n",
    "        positive_per_ds = int(len(ds_fptn_set[key]) * percentTPFN )\n",
    "        TN_items, TP_items = [], []\n",
    "        for item in remainsentences:\n",
    "            alignment = item.split(\"\\t\")[0]\n",
    "            if \"aligned\" ==alignment:\n",
    "                TP_items.append(item+\"\\t-1\\t0.0\\tGROUND-TRUTH\\t\"+key.upper()+\"\\tTP\")\n",
    "            else:\n",
    "                TN_items.append(item+\"\\t-1\\t0.0\\tGROUND-TRUTH\\t\"+key.upper()+\"\\tTN\")\n",
    "        \n",
    "        positives.extend(random.choices(TP_items, k=min(int(positive_per_ds/2), len(TP_items))))\n",
    "        positives.extend(random.choices(TN_items, k=min(int(positive_per_ds/2), len(TN_items))))                \n",
    "    return positives    \n",
    "\n",
    "to_write = []\n",
    "fix_crf_file_name(path_to_files)\n",
    "negatives = get_negatives(path_to_files)\n",
    "positives = get_positives(path_to_files)\n",
    "to_write.extend(negatives)\n",
    "to_write.extend(positives)\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(ROOT_PATH,BENCHMARK_FOLDER_NAME,\"benchmark_doublets.out\"), 'w', encoding='utf-8') as file_handler:\n",
    "    for item in to_write:\n",
    "        file_handler.write(\"{}\\n\".format(item))   \n",
    "\n",
    "ids_to_remove = []\n",
    "final_set = set()\n",
    "with open(os.path.join(ROOT_PATH,BENCHMARK_FOLDER_NAME,\"benchmark_doublets.out\"), 'r', encoding='utf-8') as f:\n",
    "    for index, line in enumerate(f):\n",
    "        items = line.split(\"\\t\")   \n",
    "        key = items[0]+\"\\t\"+items[1]+\"\\t\"+items[2]\n",
    "        if key in final_set:\n",
    "           ids_to_remove.append(index)\n",
    "        else: \n",
    "            final_set.add(key)\n",
    "print(f'Found {len(ids_to_remove)} to remove') \n",
    "\n",
    "with open(os.path.join(ROOT_PATH,BENCHMARK_FOLDER_NAME,\"benchmark.out\"), 'w', encoding='utf-8') as file_handler:\n",
    "    for index, item in enumerate(to_write):\n",
    "        if index not in ids_to_remove:\n",
    "            file_handler.write(\"{}\\n\".format(item))   \n",
    "       \n",
    "\n",
    "count_errors = 0\n",
    "ds = {}\n",
    "with open(os.path.join(ROOT_PATH,BENCHMARK_FOLDER_NAME,\"benchmark.out\"), 'r', encoding='utf-8') as f:\n",
    "    for index, line in enumerate(f):\n",
    "        items = line.split(\"\\t\")   \n",
    "        if len(items)!=8:\n",
    "            print(f'Error in {index} - {len(items)} - {line}')\n",
    "            count_errors+=1\n",
    "        if items[-2] not in ds:\n",
    "                ds[items[-2]] = 0\n",
    "        ds[items[-2]] +=1\n",
    "\n",
    "print(\"DS stats\")    \n",
    "print(f'Total nbr of records: {index}')\n",
    "for key in ds:\n",
    "    print(f'\\t{key} -> {ds[key]}')\n",
    "\n",
    "print(f'Nbr of errors: {count_errors}')\n",
    "print(f'Benchmark done ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygeometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
